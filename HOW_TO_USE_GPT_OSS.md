# C√≥mo Usar GPT OSS 120B en tu Aplicaci√≥n

## Acceso Inmediato

El modelo ya est√° disponible y listo para usar. No requiere configuraci√≥n adicional.

### Opci√≥n 1: Desde la Interfaz Web

1. **Abre tu aplicaci√≥n en el navegador**
   - Navega a tu sitio

2. **En la pantalla de chat, busca el selector de modelos**
   - Generalmente est√° en la parte superior o en un dropdown

3. **Selecciona "GPT OSS 120B"**
   - Aparecer√° en la secci√≥n de modelos GRATUITOS
   - Muestra: ‚ú® (razonamiento habilitado)

4. **(Opcional) Activa "Razonamiento Avanzado"**
   - Si tu UI lo soporta, ver√°s un toggle de "reasoning"
   - Esto habilita el pensamiento paso a paso

5. **Escribe tu mensaje y presiona enviar**
   - Ejemplo: "Resuelve este problema matem√°tico complicado"
   - El modelo procesar√° tu solicitud con el contexto de 131K tokens

### Opci√≥n 2: Solicitud API Directa

```bash
# Asume que tienes un token de autenticaci√≥n
TOKEN="tu_bearer_token_aqu√≠"

curl -X POST http://localhost:5000/api/chat \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "conversationId": "tu-conversation-id",
    "message": "¬øCu√°l es la mejor soluci√≥n arquitect√≥nica para este problema?",
    "model": "gpt-oss-120b",
    "useReasoning": true,
    "chatMode": "general"
  }' \
  --no-buffer
```

**Respuesta en streaming SSE:**
```
data: {"conversationId":"...", "requestId":"..."}
data: {"thinking":"Primero debo entender..."}
data: {"content":"La mejor soluci√≥n ser√≠a..."}
data: [DONE]
```

## Casos de Uso Recomendados

### 1. Problemas Complejos con Razonamiento
```
"Necesito optimizar un algoritmo de ordenamiento para N=1,000,000 items. 
¬øCu√°l es la mejor estrategia? Explica tu razonamiento paso a paso."

useReasoning: true
```
‚Üí Ver√°s el pensamiento del modelo antes de la respuesta

### 2. An√°lisis de Documentos Largos
```
"Aqu√≠ va un documento de 30,000 palabras... ¬øCu√°les son los puntos clave?"
```
‚Üí Aprovecha los 131K tokens para documentos enormes

### 3. Consultas Multiling√ºes
```
"Can you analyze this code AND explicar en espa√±ol c√≥mo optimizarlo?"
```
‚Üí Excelente en 81+ idiomas simult√°neamente

### 4. B√∫squeda Web + An√°lisis
```
"¬øCu√°les son las √∫ltimas tendencias en IA? Busca informaci√≥n reciente."

useWebSearch: true
```
‚Üí Combina b√∫squeda web con an√°lisis profundo

### 5. Programaci√≥n Competitiva
```
"Resuelve este problema de LeetCode Hard en O(n log n)"
```
‚Üí 62.4% en SWE-Bench, mejor que muchos otros modelos

## Diferencias vs Otros Modelos en tu App

| Caracter√≠stica | GPT OSS 120B | Llama 3.3 | Qwen Coder |
|---|---|---|---|
| **Velocidad** | 500 tps | 500 tps | M√°s lento |
| **Contexto** | 131K | 128K | 262K |
| **Razonamiento** | ‚úÖ S√≠ | ‚ùå No | ‚ùå No |
| **Multiling√ºe** | ‚úÖ 81 idiomas | ‚ö†Ô∏è Bueno | ‚ö†Ô∏è Limitado |
| **Programaci√≥n** | ‚úÖ Muy bueno | ‚úÖ Muy bueno | ‚úÖ Especializado |
| **Costo** | Econ√≥mico | Econ√≥mico | Econ√≥mico |

**Cu√°ndo usar cada uno:**
- **GPT OSS 120B:** Problemas generales, razonamiento, multiling√ºe
- **Llama 3.3:** Programaci√≥n, uso general, conversaci√≥n r√°pida
- **Qwen Coder:** Problemas SOLO de programaci√≥n, m√°ximo contexto

## Configuraci√≥n Recomendada

### Para Mejor Razonamiento
```json
{
  "model": "gpt-oss-120b",
  "useReasoning": true,
  "temperature": 0.3,
  "chatMode": "general"
}
```
- Temperatura baja = respuestas m√°s directas y razonadas
- Razonamiento habilitado = pensamiento profundo

### Para Respuestas R√°pidas
```json
{
  "model": "gpt-oss-120b",
  "useReasoning": false,
  "temperature": 0.7,
  "chatMode": "general"
}
```
- Sin razonamiento = 3-5x m√°s r√°pido
- Temperatura normal = respuestas m√°s creativas

### Para Documentos Enormes
```json
{
  "model": "gpt-oss-120b",
  "useReasoning": false,
  "chatMode": "general"
  // Sin razonamiento para m√°xima capacidad de contexto
}
```
- 131K tokens completos para tu documento
- Procesamiento m√°s r√°pido sin razonamiento

## Limitaciones Conocidas

| Limitaci√≥n | Detalles |
|---|---|
| **No soporta im√°genes** | Solo texto en/out |
| **Rate limiting** | ~30 req/min en plan free de Groq |
| **Razonamiento requiere presupuesto** | 5K tokens para users free |
| **Output m√°ximo** | 65K tokens (muy generoso) |

## Troubleshooting

### "Modelo no aparece en el selector"
- Reinicia la aplicaci√≥n
- Limpia cach√© del navegador (Ctrl+Shift+Del)
- Verifica que tu navegador est√° actualizado

### "Error de API: La clave no est√° configurada"
- Contacta al administrador
- Necesita variable de entorno `grokAPI` en servidor

### "Razonamiento no funciona"
- Solo funciona si `supportsReasoning: true` para el modelo
- Requiere `useReasoning: true` en tu solicitud
- Puede ser lento (es normal, necesita pensar)

### "Respuesta cortada o incompleta"
- Est√°s cerca del l√≠mite de 65K tokens de output
- Reduce el contexto anterior (limita a √∫ltimos 10 mensajes)
- O usa Llama 3.3 que tiene 32K pero es m√°s eficiente

## Ventajas Principales

1. **Ultra R√°pido en Groq** (~500 tps)
2. **Muy Econ√≥mico** (~$0.0005 por respuesta t√≠pica)
3. **Contexto Enorme** (131K para documentos largos)
4. **Razonamiento Avanzado** (mismo nivel que modelos caros)
5. **Multiling√ºe** (81+ idiomas)
6. **Disponible para FREE users** (sin tier premium)

## Estad√≠sticas del Modelo

```
Velocidad en Groq:    500 tokens/segundo ‚ö°
Contexto:             131,072 tokens (131K) üìö
Output m√°ximo:        65,536 tokens (65K) üìù
Par√°metros activos:   5.1B por token üß†
Par√°metros totales:   120B üîß

Benchmarks:
  MMLU (razonamiento):  90.0% ‚úÖ
  SWE-Bench (c√≥digo):   62.4% ‚úÖ
  MMMLU (multiling√ºe):  81.3% ‚úÖ
```

## Preguntas Frecuentes

**¬øEs realmente gratis?**
- S√≠, cualquier usuario puede usarlo
- Solo pagas si usas token masivamente (1M+ tokens/mes)
- T√≠picamente: $0.0005 por respuesta

**¬øQu√© tan bueno es para programaci√≥n?**
- 62.4% en SWE-Bench (muy competitivo)
- Mejor que algunos modelos premium
- Excelente para debugging y refactoring

**¬øPuedo usarlo para aplicaciones en producci√≥n?**
- S√≠, totalmente
- Groq es confiable y r√°pido
- Costo muy bajo para vol√∫menes altos

**¬øQu√© pasa si agot√© los 131K tokens?**
- Las respuestas estar√°n truncadas
- Reduce el contexto anterior o divisi√≥n en m√∫ltiples chats
- Llama 3.3 tiene 128K pero es m√°s compacto

---

**¬°Listo! Tu modelo GPT OSS 120B est√° operativo y funcional.**
