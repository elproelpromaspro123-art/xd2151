# Gemini 3 Pro Preview - Ejemplos de Uso

## Ejemplos de Solicitudes

### 1. Chat B√°sico de Texto
```javascript
// Cliente (React)
const response = await fetch('/api/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        conversationId: 'conv-123',
        message: '¬øCu√°l es la capital de Espa√±a?',
        model: 'gemini-3-pro-preview',
        useReasoning: false,
        chatMode: 'general'
    })
});

// Response (SSE Stream)
// data: {"conversationId":"conv-123","requestId":"req-456"}
// data: {"delta":"La","type":"content"}
// data: {"delta":" capital","type":"content"}
// ...
```

### 2. Chat con Razonamiento Avanzado
```javascript
// Solicitud con pensamiento mejorado
const response = await fetch('/api/chat', {
    method: 'POST',
    body: JSON.stringify({
        conversationId: 'conv-123',
        message: 'Resuelve este problema complejo: Si un tren viaja a 100 km/h y otro a 80 km/h saliendo del mismo punto...',
        model: 'gemini-3-pro-preview',
        useReasoning: true,  // Habilita Extended Thinking (8K tokens)
        chatMode: 'general'
    })
});

// El modelo usar√° 8,000 tokens para razonar internamente
// Antes de generar la respuesta
```

### 3. An√°lisis de Imagen
```javascript
// Convertir imagen a base64
const canvas = document.createElement('canvas');
const ctx = canvas.getContext('2d');
const img = new Image();
img.onload = () => {
    ctx.drawImage(img, 0, 0);
    const imageBase64 = canvas.toDataURL('image/png');
    
    // Enviar al chat
    const response = await fetch('/api/chat', {
        method: 'POST',
        body: JSON.stringify({
            conversationId: 'conv-123',
            message: '¬øQu√© ves en esta imagen?',
            model: 'gemini-3-pro-preview',
            imageBase64: imageBase64,  // Data URL con base64
            chatMode: 'general'
        })
    });
};
img.src = '/path/to/image.jpg';
```

### 4. B√∫squeda en Tiempo Real
```javascript
// El sistema detecta autom√°ticamente intenci√≥n de b√∫squeda
const response = await fetch('/api/chat', {
    method: 'POST',
    body: JSON.stringify({
        conversationId: 'conv-123',
        message: '¬øCu√°les son las √∫ltimas noticias sobre IA?',  // Detecci√≥n autom√°tica
        model: 'gemini-3-pro-preview',
        useWebSearch: true,  // O detecci√≥n autom√°tica
        chatMode: 'general'
    })
});

// Response incluye:
// data: {"webSearchUsed":true,"webSearchDetected":true}
// + Resultados de b√∫squeda integrados
```

### 5. Ejecuci√≥n de C√≥digo Python
```javascript
// El modelo puede ejecutar c√≥digo Python
const response = await fetch('/api/chat', {
    method: 'POST',
    body: JSON.stringify({
        conversationId: 'conv-123',
        message: 'Escribe un script Python que calcule los primeros 10 n√∫meros de Fibonacci',
        model: 'gemini-3-pro-preview',
        chatMode: 'general'
    })
});

// Respuesta incluir√°:
// - C√≥digo Python ejecutable
// - Explicaci√≥n del proceso
// - Salida esperada
```

### 6. An√°lisis de Video (Conceptual)
```javascript
// Aunque streaming de video no es directo en webapp,
// se puede procesar como frames
const video = document.getElementById('myVideo');
const canvas = document.createElement('canvas');

// Capturar frame
const ctx = canvas.getContext('2d');
ctx.drawImage(video, 0, 0);
const frameBase64 = canvas.toDataURL('image/png');

const response = await fetch('/api/chat', {
    method: 'POST',
    body: JSON.stringify({
        conversationId: 'conv-123',
        message: 'Analiza este frame de video',
        model: 'gemini-3-pro-preview',
        imageBase64: frameBase64,
        chatMode: 'general'
    })
});
```

### 7. Procesamiento de Documentos
```javascript
// Convertir PDF a base64 (requiere librer√≠a como pdf.js)
async function pdfToBase64(file) {
    const arrayBuffer = await file.arrayBuffer();
    const bytes = new Uint8Array(arrayBuffer);
    const binaryString = String.fromCharCode(...bytes);
    return 'data:application/pdf;base64,' + btoa(binaryString);
}

const pdfBase64 = await pdfToBase64(document.getElementById('pdfFile').files[0]);

const response = await fetch('/api/chat', {
    method: 'POST',
    body: JSON.stringify({
        conversationId: 'conv-123',
        message: 'Resumen el contenido de este PDF',
        model: 'gemini-3-pro-preview',
        imageBase64: pdfBase64,  // API soporta PDF como data URL
        chatMode: 'general'
    })
});
```

### 8. Regeneraci√≥n de Respuesta
```javascript
// Si la respuesta anterior no fue satisfactoria
const response = await fetch('/api/chat/regenerate', {
    method: 'POST',
    body: JSON.stringify({
        conversationId: 'conv-123',
        lastUserMessage: '¬øCu√°l es la capital de Espa√±a?',
        model: 'gemini-3-pro-preview',
        useReasoning: true,
        chatMode: 'general'
    })
});
```

## Respuestas del Servidor (SSE Stream)

### Evento de Inicializaci√≥n
```json
{
    "conversationId": "conv-123",
    "requestId": "req-456",
    "webSearchUsed": false,
    "webSearchDetected": false
}
```

### Eventos de Contenido
```json
{"delta": "Texto ", "type": "content"}
{"delta": "de ", "type": "content"}
{"delta": "respuesta", "type": "content"}
```

### Eventos de Pensamiento (Si est√° habilitado)
```json
{
    "delta": "Pensemos en esto...",
    "type": "thinking"
}
```

### Evento de Finalizaci√≥n
```json
{
    "type": "done",
    "finishReason": "STOP",
    "totalTokensUsed": 1523
}
```

## L√≠mites de Uso en Free

```
üìä L√çMITES DE TOKENS (Free)
‚îú‚îÄ Entrada: 943,718 tokens
‚îú‚îÄ Salida: 58,982 tokens
‚îú‚îÄ Thinking: 8,000 tokens
‚îî‚îÄ Total: ~1,010,700 tokens/solicitud

‚è±Ô∏è L√çMITES DE MENSAJES
‚îú‚îÄ Modo General: 10 mensajes/3 d√≠as
‚îú‚îÄ Modo Roblox: 10 mensajes/3 d√≠as
‚îî‚îÄ B√∫squeda Web: 5 b√∫squedas/3 d√≠as
```

## Optimizaciones para Free

### 1. Reducir Presupuesto de Thinking
```javascript
// En lugar de usar thinking para todo:
useReasoning: true  // 8K tokens

// Solo usar para problemas complejos
const isComplex = message.length > 500 || containsMath(message);
useReasoning: isComplex;
```

### 2. Limitar Contexto
El sistema autom√°ticamente mantiene solo los √∫ltimos 20 mensajes para optimizar tokens.

### 3. B√∫squeda Web Selectiva
```javascript
const keywords = ['√∫ltima', 'nuevo', 'reciente', 'actual', 'hoy', 'ahora'];
const shouldSearch = keywords.some(kw => message.toLowerCase().includes(kw));
useWebSearch: shouldSearch;
```

## M√©tricas de Rendimiento

### Velocidad de Generaci√≥n (T√≠pica)
- **Sin Razonamiento**: 50-100 tokens/seg
- **Con Razonamiento**: 30-50 tokens/seg

### Latencia Inicial
- **Inicio de conexi√≥n**: <500ms
- **Primer token**: 1-2 segundos
- **Streaming**: Continuo

### Consumo de Tokens (T√≠pico)
- **Pregunta simple**: 50-150 tokens
- **Pregunta con contexto**: 200-500 tokens
- **Con razonamiento**: +8,000 tokens
- **Con b√∫squeda web**: +200-300 tokens

## Manejo de Errores

### Error de Rate Limit
```javascript
// El servidor retorna autom√°ticamente:
{
    error: "L√≠mite de rate limit alcanzado. Espera aproximadamente 24 horas.",
    code: "RATE_LIMIT_EXCEEDED"
}

// Cliente debe mostrar contador en tiempo real
```

### Error de Modelo No Disponible
```javascript
{
    error: "Este modelo requiere una cuenta Premium.",
    code: "PREMIUM_REQUIRED"
}
```

### Error de L√≠mite de Mensajes
```javascript
{
    error: "Has alcanzado el l√≠mite de mensajes para el modo General. Los l√≠mites se reinician cada 3 d√≠as.",
    code: "MESSAGE_LIMIT_REACHED"
}
```

## Checklist de Integraci√≥n

- ‚úÖ Modelo agregado a `AI_MODELS` en `routes.ts`
- ‚úÖ Tokens configurados para Free y Premium
- ‚úÖ Soporte multimodal habilitado
- ‚úÖ Razonamiento avanzado configurado
- ‚úÖ B√∫squeda integrada
- ‚úÖ Ejecuci√≥n de c√≥digo habilitada
- ‚úÖ Rate limit tracking implementado
- ‚úÖ Notificaciones SSE en tiempo real
- ‚úÖ Documentaci√≥n completa
- ‚úÖ Ejemplos funcionales

## Pr√≥ximos Pasos

1. Probar con solicitudes de prueba
2. Monitorear uso en producci√≥n
3. Ajustar l√≠mites seg√∫n necesidad real
4. Recopilar feedback de usuarios
5. Optimizar par√°metros de generaci√≥n
